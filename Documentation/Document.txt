Infosys Internship Program Documentation
Project: Visualizing US Natural Disaster Declarations

Intern Name: Onkar Devkar  
Mentor: Akshay sir
Program: Data Visualization Internship - Infosys

---

Program Overview

This internship is a comprehensive, hands-on data analysis program focused on real-world project execution through the visualization and analysis of US natural disaster declarations. The program emphasizes collaborative problem-solving, mentored guidance, and practical application of data science methodologies in a professional environment.

Program Structure
- Duration: 8 weeks intensive program
- Format: Daily collaborative sessions 
- Approach: Learning by doing with immediate feedback and iterative improvement
- Focus: Real-world data challenges and professional development practices

Professional Standards
- Punctuality and consistent attendance
- Proactive communication via email for all official matters
- Prior notification required for leave (otherwise marked absent)
- High professional decorum and respectful collaboration
- Quality deliverables and thorough documentation
- Commitment to continuous learning and improvement

---

Week 1: Foundation Building & Internship Overview

Overview
The first week establishes the foundation for the entire internship through team formation, project orientation, and alignment of expectations. This week sets the tone for professional conduct, establishes working relationships, and provides a comprehensive understanding of the project scope and deliverables.

Learning Objectives
- Understand complete project scope and deliverable expectations
- Align on professional standards and communication protocols
- Establish team dynamics and collaboration strategies
- Set up development environment
- Identify knowledge gaps and learning goals
- Understand the significance of natural disaster data analysis

Key Topics Covered

Project Introduction:
- Understanding US natural disaster declarations and their importance
- Historical context of FEMA disaster declarations
- Real-world applications of disaster data analysis
- Impact on emergency management and resource allocation
- Climate patterns and trends analysis

Team Alignment:
- Individual introductions and background sharing
- Learning goals and expectations discussion
- Role clarification and responsibility assignment
- Collaboration tools and communication channels
- Ground rules for effective teamwork

Key Outcomes
- Clear understanding of 8-week roadmap
- Established team support and trust
- Aligned expectations between mentor and interns
- Prepared mindset for technical work ahead
- Foundation for professional collaboration

---

Week 2: Data Quality Management with Python and Power BI

Overview
Week 2 focuses on data quality management using both Python and Power BI. Participants learn systematic approaches to identify, assess, and resolve data quality issues while building interactive dashboards for data quality monitoring and reporting.

Learning Objectives
- Master data cleaning techniques using Python (Pandas, NumPy)
- Understand data quality dimensions and assessment methods
- Implement data validation and quality checks
- Create data quality dashboards in Power BI
- Develop reproducible data cleaning pipelines
- Document data cleaning decisions and transformations

Key Topics Covered

Data Quality Fundamentals:
- Understanding data quality dimensions (completeness, consistency, accuracy, validity, uniqueness, timeliness)
- Impact of poor data quality on analysis and decision-making
- Data profiling and quality assessment techniques
- Establishing data quality metrics and KPIs
- Quality assurance best practices

Python for Data Cleaning:
- Pandas library for data manipulation
- Handling missing values (deletion, imputation strategies)
- Duplicate detection and removal
- Data type conversions and standardization( like a use the .strip() method to remove extra characters)
- Outlier detection and treatment methods
- String manipulation and text cleaning
- Date/time parsing and formatting
- Merging and joining datasets
- Data transformation techniques

Power BI for Data Quality Management:
- Introduction to Power BI Desktop
- Connecting to various data sources
- Power Query Editor for data transformation ( In this step we learn the Data cleaning using Power Query)
- Creating calculated columns and measures using DAX
- Building data quality dashboards
- Visualizing missing data patterns
- Tracking data quality metrics over time
- Interactive filtering and drill-down capabilities
- Publishing and sharing reports

Documentation Practices:
- Maintaining data cleaning logs
- Documenting transformation logic
- Creating data dictionaries
- Version control for data pipelines
- Reproducibility and auditability

Practical Applications
- Cleaning US natural disaster dataset
- Identifying and resolving real data quality issues
- Building Power BI dashboard for data quality monitoring
- Creating automated validation scripts
- Establishing data quality standards for the project

Tools & Technologies
- Python (Pandas, NumPy, Matplotlib)
- Jupyter Notebooks
- Power BI Desktop
- Power Query and DAX
- Excel for data profiling

---

Week 3: Exploratory Data Analysis (EDA)

Overview
Week 3 transitions from data preparation to data understanding through comprehensive Exploratory Data Analysis. Participants learn to explore, visualize, and extract meaningful insights from the cleaned disaster dataset using statistical and visual techniques.

Learning Objectives
- Master statistical analysis techniques for data exploration
- Create effective visualizations revealing patterns and insights
- Identify relationships and correlations between variables
- Recognize temporal and spatial patterns in disaster data
- Formulate and test data-driven hypotheses
- Communicate findings through visual storytelling
- Understand disaster trends and patterns across US

Key Topics Covered

Statistical Analysis:
- Descriptive statistics (mean, median, mode, standard deviation, variance)
- Distribution analysis and normality testing
- Correlation and covariance analysis
- Statistical significance testing
- Confidence intervals and hypothesis testing
- Time series analysis fundamentals

Univariate Analysis:
- Analyzing individual variables in isolation
- Frequency distributions for categorical data
- Numerical data distribution patterns
- Identifying central tendency and spread
- Detecting skewness and kurtosis
- Box plots and outlier visualization
- Histograms and density plots

Bivariate Analysis:
- Exploring relationships between variable pairs
- Scatter plots for numerical relationships
- Correlation coefficients interpretation
- Cross-tabulation for categorical variables
- Grouped comparisons and contrasts
- Trend identification and pattern recognition

Business-Oriented Understanding using Sales Data:
- EDA is not just charts and statistics.

From a business perspective, EDA answers one core question:
- ‚ÄúWhat is actually happening in my business, and why?‚Äù

Before any ML model or dashboard, EDA helps decision-makers understand:
- Where money is coming from
- What products/regions/customers are driving performance
- What risks or opportunities exist
- What trends are emerging over time

Think of EDA as business diagnosis üîç.

---

Week 4: Live Data Import, Zendesk Overview, APIs & Git Repository

Overview
The final week integrates professional data engineering practices including live data imports, API integration, Zendesk platform overview for ticketing systems, and comprehensive GitHub repository management. Participants learn to build production-ready data pipelines and collaborate using industry-standard tools.

Learning Objectives
- Connect to and import data from live APIs
- Understand Zendesk platform and its data structure
- Automate data retrieval and pipeline processes
- Master Git and GitHub for version control
- Implement professional code organization
- Create comprehensive project documentation
- Integrate all project components into cohesive workflow

Key Topics Covered

API Fundamentals:
- Understanding RESTful API concepts
- HTTP methods (GET, POST, PUT, DELETE)
- API authentication (API keys, OAuth tokens)
- Request headers and parameters
- JSON response parsing
- Error handling and status codes
- Rate limiting and pagination
- API documentation interpretation

Live Data Import:
- FEMA OpenFEMA API integration
- NOAA Climate Data API
- US Census Bureau API
- Automated data extraction scripts
- Incremental data updates
- Data validation after import
- Handling API failures gracefully
- Scheduling automated data pulls

Zendesk Platform Overview:
- Introduction to Zendesk ticketing system
- Understanding tickets, users, and organizations
- Zendesk data structure and relationships
- Zendesk API capabilities
- Extracting ticket data and metrics
- Support analytics and reporting
- Integration with data analysis workflows
- Use cases in customer support analytics

Python for API Integration:
- Requests library for HTTP requests
- Authentication implementation
- Parsing JSON responses
- Converting API data to DataFrames
- Error handling and retry logic
- Asynchronous requests for performance
- Building reusable API wrapper functions

Git & GitHub Mastery:
- Version control fundamentals
- Git workflow (add, commit, push, pull)
- Repository initialization and cloning
- Branching strategies and merging
- Handling merge conflicts
- Pull requests and code reviews
- Collaborative development workflows
- GitHub features (Issues, Projects, Wiki)

Project Repository Organization:
- Professional folder structure design
	1. Data
	2. Power BI
	3. Data cleaning
	4. Visuals screenshot
	5. Project Documentation 
- Separating code, data, and documentation
- README.md best practices
- Requirements.txt and dependency management
- Documentation standards
- Code commenting and docstrings
- Reproducibility considerations

Data Pipeline Integration:
- End-to-end workflow automation
- Connecting data import, cleaning, and analysis
- Configuration management
- Logging and monitoring
- Error notification systems
- Pipeline testing and validation
- Performance optimization
- Individual reflection report

Tools & Technologies
- Python Requests library
- Git and GitHub
- Zendesk API
- FEMA, NOAA, Census APIs
- Cron/Task Scheduler for automation
- Virtual environments
- Markdown for documentation

---

Overall Skills Developed

Technical Skills
- Python programming for data science
- Data cleaning and quality management
- Exploratory Data Analysis (EDA)
- Statistical analysis and interpretation
- Data visualization (Python and Power BI)
- API integration and automation
- Version control with Git/GitHub
- Dashboard development
- Data pipeline creation

Analytical Skills
- Data quality assessment
- Pattern recognition
- Statistical reasoning
- Hypothesis formulation and testing
- Critical thinking about data
- Domain knowledge application
- Insight extraction and communication

Professional Skills
- Project documentation
- Collaborative development
- Time management
- Professional communication
- Code review practices
- Meeting deadlines
- Self-directed learning
- Presentation skills

---

Tools & Technologies Used

- Programming: Python, SQL
- Libraries: Pandas, NumPy, Matplotlib, Seaborn, Plotly, Requests
- BI Tools: Power BI Desktop, Power Query, DAX
- Development: VS Code, Jupyter Notebooks
- Version Control: Git, GitHub
- APIs: FEMA OpenFEMA, NOAA, Census Bureau, Zendesk
- Platforms: Zendesk (ticketing system overview)

---

Project Outcomes

This internship provides hands-on experience with real-world data science workflows, from data acquisition through analysis to presentation. Participants develop both technical capabilities and professional practices essential for data science careers, including data quality management, analytical thinking, automation skills, collaborative development, and effective communication of insights.

---

Document Prepared By: Onkar Devkar 
Mentor: Akshay Sir
Program: Infosys Internship - Data Visualization 
